{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distil_gpt2_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZZUPBrYYhxB+CWgzCz0ZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zinzin2312/abschlussarbeit/blob/main/distil_gpt2_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3FKLXR4CREM"
      },
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install datasets \n",
        "\n",
        "from datasets import load_dataset\n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4TA_weHD3Ue"
      },
      "source": [
        "# Classification using GPT-2 as generator and BERT as classficator\n",
        "from transformers import GPT2Tokenizer, DistilBertTokenizer, TFGPT2LMHeadModel, TFDistilBertForSequenceClassification\n",
        "\n",
        "# Preparing the pre-trained model\n",
        "ckpt_gen = \"distilgpt2\"\n",
        "ckpt_class = \"distilbert-base-uncased\"\n",
        "\n",
        "tokenizer_gen = GPT2Tokenizer.from_pretrained(ckpt_gen)\n",
        "tokenizer_class = DistilBertTokenizer.from_pretrained(ckpt_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP4b4pLGZLPW",
        "outputId": "1e9ede00-f582-43f8-b358-800c2583b51d"
      },
      "source": [
        "# Download models\n",
        "\n",
        "# Note that to be able to use models out-of-the-box we need a corresponding head for each task\n",
        "model_gen = TFGPT2LMHeadModel.from_pretrained(ckpt_gen)\n",
        "model_class = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "    ckpt_class,\n",
        "    # Specify number of labels\n",
        "    num_labels=2\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_38']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BiKRcOfrkdL"
      },
      "source": [
        "# model_gen.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmTbJyXwrtHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9bd720-be2d-4300-f59e-b87c31dd7843"
      },
      "source": [
        "model_class.config.id2label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'LABEL_0', 1: 'LABEL_1'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqsxDg3RFkAe",
        "outputId": "372a839f-798c-49a4-dcbc-1bcf0ee73358"
      },
      "source": [
        "# sms_spam dataset: https://huggingface.co/datasets/sms_spam\n",
        "# Structure : sms_spam dict{'label', 'sms'}\n",
        "# Length: 5574\n",
        "\n",
        "sms_spam = load_dataset('sms_spam', split='train')\n",
        "sms = sms_spam['sms']\n",
        "\n",
        "max_word_nr = 20\n",
        "limit_size = 20\n",
        "sms = [s for s in sms if len(s.split(\" \")) <= max_word_nr][:limit_size]\n",
        "sms_labels = sms_spam['label'][:limit_size]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset sms_spam (/root/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii4I-z-wghjq"
      },
      "source": [
        "def create_batch(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "        \n",
        "# Usage\n",
        "# list(create_batch(range(10, 75), 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EASQl6QeYhoa"
      },
      "source": [
        "# Generate text data\n",
        "\n",
        "# GPT 2 doesn't have pad_token, define pad = eos\n",
        "tokenizer_gen.pad_token = tokenizer_gen.eos_token\n",
        "input_gen = tokenizer_gen(sms, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(input_gen)\n",
        "\n",
        "\"\"\" Generate text using different strats\n",
        "https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_tf_utils.TFGenerationMixin.generate\n",
        "\"\"\"\n",
        "# max_length = model_gen.config.n_ctx\n",
        "max_length = 50\n",
        "num_return_sequences = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZDg5XArmgrj",
        "outputId": "d83be754-48e8-415d-85b3-63ef030a21b0"
      },
      "source": [
        "# Greedy search\n",
        "\n",
        "greedy_output = model_gen.generate(input_gen['input_ids'], max_length=max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P7SPiOqmg53",
        "outputId": "f0f84fa9-fe3d-45aa-b90b-ab7f8bbfc6be"
      },
      "source": [
        "# Beam search\n",
        "\n",
        "beam_outputs = model_gen.generate(\n",
        "    input_gen['input_ids'],\n",
        "    max_length=max_length, \n",
        "    num_beams=5, \n",
        "    # penalizes repetitive n_grams\n",
        "    # use with caution, might deletes name of City in article etc.\n",
        "    no_repeat_ngram_size=2,\n",
        "    # penalizes repetitive words\n",
        "    # repetition_penalty = 1.5, \n",
        "    early_stopping=True\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOmwJ2Q7vBIU"
      },
      "source": [
        "import tensorflow as tf\n",
        "# Changing seed randomizes the results\n",
        "tf.random.set_seed(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I58INYsbmhDQ",
        "outputId": "30c91aed-a1ad-4ae6-bf5d-43617b2cf508"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "# with temp = 0 sampling becomes greedy\n",
        "sample_output = model_gen.generate(\n",
        "    input_gen['input_ids'],\n",
        "    do_sample=True, \n",
        "    max_length=max_length, \n",
        "    top_k=0,\n",
        "    temperature=0.7\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlEVVIFLoWhK",
        "outputId": "1897c039-e256-41b7-89b5-8114f41adf18"
      },
      "source": [
        "# Top-K Sampling\n",
        "\n",
        "sample_top_k_outputs = model_gen.generate(\n",
        "    input_gen['input_ids'],\n",
        "    do_sample=True, \n",
        "    max_length=max_length, \n",
        "    # set top_k to 50\n",
        "    top_k=50,\n",
        "    num_return_sequences=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxZhQMgZqEdq",
        "outputId": "8ad9ff33-c983-41fb-f13a-c9d03158b458"
      },
      "source": [
        "# Top-P Sampling\n",
        "\n",
        "\n",
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "sample_top_p_outputs = model_gen.generate(\n",
        "    input_gen['input_ids'],\n",
        "    do_sample=True, \n",
        "    max_length=max_length, \n",
        "    top_p=0.92, \n",
        "    top_k=0,\n",
        "    num_return_sequences=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm64zvdM8BYt"
      },
      "source": [
        "# tokenizer_gen.decode(greedy_output[0], skip_special_tokens=True)\n",
        "# print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "# for i, beam_output in enumerate(beam_outputs):\n",
        "#   print(\"{}: {}\".format(i, tokenizer_gen.decode(beam_output, skip_special_tokens=True)))\n",
        "# print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "\n",
        "# print(tokenizer_gen.decode(sample_output[0], skip_special_tokens=True))\n",
        "# print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "\n",
        "# for i, sample_output in enumerate(sample_top_k_outputs):\n",
        "#   print(\"{}: {}\".format(i, tokenizer_gen.decode(sample_output, skip_special_tokens=True)))\n",
        "# print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "\n",
        "# for i, so in enumerate(sample_top_p_outputs):\n",
        "#   print(\"{}: {}\".format(i, tokenizer_gen.decode(so, skip_special_tokens=True)))\n",
        "\n",
        "outputs = sample_top_p_outputs\n",
        "syn_sms = []\n",
        "for output in outputs:\n",
        "  syn_sms.append(tokenizer_gen.decode(output, skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVqqA0beHW2k"
      },
      "source": [
        "data = sms\n",
        "syn_data = syn_sms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hzoru3pFTvf"
      },
      "source": [
        "# F1 score metric?\n",
        "# Doesn't work ????\n",
        "class F1_metric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        # Initialize our metric by initializing the two metrics it's based on:\n",
        "        # Precision and Recall\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Update our metric by updating the two metrics it's based on\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.precision.reset_state()\n",
        "        self.recall.reset_state()\n",
        "\n",
        "    def result(self):\n",
        "        # To get the F1 result, we compute the harmonic mean of the current\n",
        "        # precision and recall\n",
        "        return 2 / ((1 / self.precision.result()) + (1 / self.recall.result())) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eobuIAx0GlTw"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Learning rate scheduling\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 20\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs\n",
        "num_train_steps = (len(data) // batch_size) * num_epochs\n",
        "lr_scheduler = PolynomialDecay(\n",
        "    initial_learning_rate=5e-5,\n",
        "    end_learning_rate=0.,\n",
        "    decay_steps=num_train_steps\n",
        "    )\n",
        "\n",
        "# customized optimizer version of Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "opt = Adam(learning_rate=lr_scheduler)\n",
        "\n",
        "# Defining loss funct to be calculated from logits\n",
        "# Always check if loss funct matches model outputting logits or probs\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# model_class.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Compile the model\n",
        "model_class.compile(optimizer=opt,\n",
        "                    loss=loss,\n",
        "                    # Passing metrics on the fly to observe how the model is doing\n",
        "                    # has to be passed as list\n",
        "                    metrics=['accuracy']\n",
        "                    # Doesn't work???\n",
        "                    # metrics=['accuracy', F1_metric()]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8r7QDzBr8pq"
      },
      "source": [
        "# print(syn_sms[0])\n",
        "# tokenizer_class.convert_ids_to_tokens(batch['input_ids'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oANdQZcJtPW1",
        "outputId": "541d1823-8d37-4723-b681-9fed7f581a04"
      },
      "source": [
        "# Train the model with OG dataset\n",
        "og_batch = dict(tokenizer_class(data, padding=True, truncation=True, return_tensors=\"tf\"))\n",
        "labels = tf.convert_to_tensor(sms_labels)\n",
        "\n",
        "# model_class.train_on_batch(batch, labels)\n",
        "\n",
        "model_class.fit(\n",
        "    og_batch,\n",
        "    labels,\n",
        "    validation_data=(og_batch, labels),\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3/3 [==============================] - 15s 1s/step - loss: 0.7069 - accuracy: 0.5000 - val_loss: 0.6549 - val_accuracy: 0.6000\n",
            "Epoch 2/20\n",
            "3/3 [==============================] - 1s 202ms/step - loss: 0.6621 - accuracy: 0.6000 - val_loss: 0.6344 - val_accuracy: 0.6000\n",
            "Epoch 3/20\n",
            "3/3 [==============================] - 1s 200ms/step - loss: 0.6366 - accuracy: 0.6000 - val_loss: 0.5922 - val_accuracy: 0.6000\n",
            "Epoch 4/20\n",
            "3/3 [==============================] - 1s 198ms/step - loss: 0.5695 - accuracy: 0.6000 - val_loss: 0.5182 - val_accuracy: 0.6000\n",
            "Epoch 5/20\n",
            "3/3 [==============================] - 1s 199ms/step - loss: 0.5209 - accuracy: 0.6000 - val_loss: 0.4459 - val_accuracy: 0.6500\n",
            "Epoch 6/20\n",
            "3/3 [==============================] - 1s 202ms/step - loss: 0.4451 - accuracy: 0.7000 - val_loss: 0.3766 - val_accuracy: 0.9000\n",
            "Epoch 7/20\n",
            "3/3 [==============================] - 1s 201ms/step - loss: 0.3722 - accuracy: 0.9500 - val_loss: 0.3156 - val_accuracy: 0.9500\n",
            "Epoch 8/20\n",
            "3/3 [==============================] - 1s 198ms/step - loss: 0.3091 - accuracy: 1.0000 - val_loss: 0.2538 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "3/3 [==============================] - 1s 201ms/step - loss: 0.2563 - accuracy: 0.9500 - val_loss: 0.2120 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "3/3 [==============================] - 1s 199ms/step - loss: 0.2219 - accuracy: 1.0000 - val_loss: 0.1866 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "3/3 [==============================] - 1s 201ms/step - loss: 0.1984 - accuracy: 1.0000 - val_loss: 0.1669 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "3/3 [==============================] - 1s 199ms/step - loss: 0.1825 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "3/3 [==============================] - 1s 198ms/step - loss: 0.1630 - accuracy: 1.0000 - val_loss: 0.1477 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "3/3 [==============================] - 1s 201ms/step - loss: 0.1591 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "3/3 [==============================] - 1s 206ms/step - loss: 0.1580 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "3/3 [==============================] - 1s 197ms/step - loss: 0.1574 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "3/3 [==============================] - 1s 201ms/step - loss: 0.1607 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "3/3 [==============================] - 1s 197ms/step - loss: 0.1537 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "3/3 [==============================] - 1s 201ms/step - loss: 0.1590 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "3/3 [==============================] - 1s 199ms/step - loss: 0.1592 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e1016afd0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPHyPaBfNVT3",
        "outputId": "cccee346-6a0b-4255-e6da-99a9d01d59a2"
      },
      "source": [
        "# Making predictions using og data\n",
        "\n",
        "preds = model_class.predict(og_batch['input_ids'])['logits']\n",
        "probs = tf.nn.softmax(preds)\n",
        "class_preds = np.argmax(probs, axis=1)\n",
        "# print(preds.shape, class_preds.shape)\n",
        "print(class_preds)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "tf.Tensor([0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1], shape=(20,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyvEnjAKB1D1",
        "outputId": "efa2dfd3-ade4-4db4-ad95-48f649c5991f"
      },
      "source": [
        "# Making predictions using syn data\n",
        "\n",
        "syn_batch = dict(tokenizer_class(syn_data, padding=True, truncation=True, return_tensors=\"tf\"))\n",
        "\n",
        "syn_preds = model_class.predict(syn_batch['input_ids'])['logits']\n",
        "syn_probs = tf.nn.softmax(syn_preds)\n",
        "syn_class_preds = np.argmax(syn_probs, axis=1)\n",
        "print(class_preds)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
            "tf.Tensor([0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1], shape=(20,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH_HcFf7NeYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66dc0eb-0006-42b8-937c-89e4b9c9d72f"
      },
      "source": [
        "# Benchmarking our model using metrics\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=syn_class_preds, references=labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.85, 'f1': 0.7692307692307693}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}